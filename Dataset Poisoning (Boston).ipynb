{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Detecting and Mitigating the Impact of Poisoned Data in the Boston Housing Dataset\n",
    "\n",
    "**Objective**:\n",
    "In this notebook, we explore the concept of \"data poisoning\" and its impact on machine learning models. Data poisoning refers to the intentional introduction of errors into a dataset to compromise the performance of a model trained on that data. Such attacks can be subtle and hard to detect, especially when the poisoned data points are few and carefully crafted.\n",
    "\n",
    "**Dataset**:\n",
    "We use the Boston Housing dataset, a well-known dataset in the machine learning community. It contains information about various houses in Boston and is used to predict house prices based on features like crime rate, average number of rooms per dwelling, and more.\n",
    "\n",
    "**Approach**:\n",
    "1. **Baseline Model**: We first train a neural network on the original, clean dataset to establish a performance baseline.\n",
    "2. **Poisoning the Data**: We then introduce poisoned data points into the dataset and retrain the model to observe the degradation in performance.\n",
    "3. **Detection and Mitigation**: Using the Isolation Forest algorithm, an anomaly detection technique, we attempt to identify and remove the poisoned data points. The model is then retrained on the cleaned dataset to assess recovery.\n",
    "\n",
    "**Outcome**:\n",
    "By the end of this notebook, we aim to understand the vulnerability of machine learning models to poisoned data and the effectiveness of anomaly detection techniques in mitigating such threats.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import necessary Libraries\n",
    "Libraries for data manipulation, model creation, and evaluation are imported."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-04T20:04:36.806326800Z",
     "start_time": "2023-10-04T20:04:34.211803700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. **Data Preparation**\n",
    "    - Load the Boston Housing dataset.\n",
    "    - Split the data into training, validation, and testing sets.\n",
    "    - Standardize the data.\n",
    "The Boston Housing dataset is loaded and split into training, validation, and testing sets. Standardizing the data ensures that all features have the same scale, which is essential for many machine learning algorithms, including neural networks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Fetch the Boston Housing dataset from the original source\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "X = data\n",
    "y = target\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T20:04:37.095501400Z",
     "start_time": "2023-10-04T20:04:36.809329200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. **Baseline Model Training**\n",
    "    - Define and train a neural network on the clean dataset.\n",
    "    - Evaluate the model's performance on the test set.\n",
    "A simple neural network is trained on the clean dataset to establish a performance baseline. This step helps us understand the model's expected performance without any interference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for model trained on clean data: 13.381465872586027\n"
     ]
    }
   ],
   "source": [
    "# Define a simple neural network model\n",
    "original_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "original_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define ReduceLROnPlateau and EarlyStopping callbacks\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=0)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0)\n",
    "\n",
    "callbacks = [reduce_lr, early_stopping]\n",
    "\n",
    "# Train on clean data\n",
    "original_model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=16, verbose=0, callbacks=callbacks)\n",
    "\n",
    "# Evaluate on clean data\n",
    "y_pred_clean = original_model.predict(X_test).flatten()\n",
    "mse_clean = mean_squared_error(y_test, y_pred_clean)\n",
    "print(f\"MSE for model trained on clean data: {mse_clean}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T20:04:40.372807100Z",
     "start_time": "2023-10-04T20:04:37.095501400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**MSE for model trained on clean data**: 13.381465872586027\n",
    "    - This value represents the Mean Squared Error (MSE) of the model trained on the original, clean dataset. It serves as our baseline performance metric."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. **Data Poisoning**\n",
    "    - Introduce poisoned data points into the training set.\n",
    "    - Retrain the model on the poisoned dataset.\n",
    "    - Evaluate the model's performance on the test set.\n",
    "By introducing poisoned data points (i.e., data points with intentionally incorrect labels or features), we simulate an attack on the dataset. The goal is to observe how such poisoned data can degrade the model's performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for model trained on poisoned data: 25.919913976611458\n"
     ]
    }
   ],
   "source": [
    "# Introduce poisoned data\n",
    "num_poisoned = 50\n",
    "X_mean = np.mean(X_train, axis=0)\n",
    "X_std = np.std(X_train, axis=0)\n",
    "X_poisoned_data = X_mean + 3 * X_std * np.random.rand(num_poisoned, X_train.shape[1])\n",
    "y_poisoned_data = np.array([50] * num_poisoned)\n",
    "\n",
    "X_poisoned = np.vstack([X_train, X_poisoned_data])\n",
    "y_poisoned = np.hstack([y_train, y_poisoned_data])\n",
    "\n",
    "# Define a simple neural network model\n",
    "poisoned_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "poisoned_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Retrain the neural network on poisoned data\n",
    "poisoned_model.fit(X_poisoned, y_poisoned, validation_split=0.2, epochs=50, batch_size=16, verbose=0,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "# Evaluate on poisoned data\n",
    "y_pred_poisoned = poisoned_model.predict(X_test).flatten()\n",
    "mse_poisoned = mean_squared_error(y_test, y_pred_poisoned)\n",
    "print(f\"MSE for model trained on poisoned data: {mse_poisoned}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T20:04:41.383545100Z",
     "start_time": "2023-10-04T20:04:40.375311800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**MSE for model trained on poisoned data**: 25.919913976611458\n",
    "    - The increase in MSE indicates that the model's performance has degraded due to the poisoned data. This showcases the vulnerability of machine learning models to such attacks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. **Detection of Poisoned Data**\n",
    "    - Use the Isolation Forest algorithm to detect and remove poisoned data points from the dataset.\n",
    "The Isolation Forest algorithm is an anomaly detection method that isolates anomalies instead of profiling normal data points. It's used here to detect the poisoned data points based on their feature values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9900\n",
      "Recall: 0.9802\n",
      "F1-Score: 0.9851\n"
     ]
    }
   ],
   "source": [
    "# Detect and remove poisoned data using IsolationForest\n",
    "iso_forest = IsolationForest(contamination=0.15)\n",
    "outliers = iso_forest.fit_predict(X_poisoned)\n",
    "\n",
    "# Create a ground truth label for the poisoned dataset\n",
    "# 1 for normal data and -1 for anomalies (poisoned data)\n",
    "true_labels = np.ones(X_poisoned.shape[0])\n",
    "true_labels[-num_poisoned:] = -1  # Last 'num_poisoned' samples are anomalies\n",
    "precision = precision_score(true_labels, outliers)\n",
    "recall = recall_score(true_labels, outliers)\n",
    "f1 = f1_score(true_labels, outliers)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "X_cleaned = X_poisoned[outliers == 1]\n",
    "y_cleaned = y_poisoned[outliers == 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T20:04:41.520216600Z",
     "start_time": "2023-10-04T20:04:41.386126500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Precision, Recall, and F1-Score**:\n",
    "    - These metrics provide insights into the effectiveness of the Isolation Forest in detecting poisoned data points.\n",
    "        - **Precision**: 0.9900 - Indicates a high rate of true positive detections.\n",
    "        - **Recall**: 0.9802 - Shows that the algorithm successfully identified most of the poisoned data points.\n",
    "        - **F1-Score**: 0.9851 - The harmonic mean of precision and recall, indicating a balanced performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. **Model Retraining and Evaluation**\n",
    "    - Retrain the model on the cleaned dataset.\n",
    "    - Evaluate the model's performance on the test set.\n",
    "After removing the detected poisoned data points, the model is retrained to see if its performance can be restored."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for model retrained after removing poisoned data: 14.96983561781577\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize the model\n",
    "# Define a simple neural network model\n",
    "cleaned_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "cleaned_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Retrain the model on cleaned data\n",
    "cleaned_model.fit(X_cleaned, y_cleaned, validation_split=0.2, epochs=50, batch_size=16, verbose=0, callbacks=callbacks)\n",
    "\n",
    "# Evaluate on cleaned data\n",
    "y_pred_retrained = cleaned_model.predict(X_test).flatten()\n",
    "mse_retrained = mean_squared_error(y_test, y_pred_retrained)\n",
    "print(f\"MSE for model retrained after removing poisoned data: {mse_retrained}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T20:04:43.030170900Z",
     "start_time": "2023-10-04T20:04:41.507980600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**MSE for model retrained after removing poisoned data**: 14.96983561781577\n",
    "    - After removing the poisoned data and retraining, the model's performance is almost restored to the baseline. However, there's still a slight increase in MSE, which might be due to the removal of some genuine data points or other factors in the training process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Summary and Conclusion:\n",
    "\n",
    "Throughout this notebook, we attempt to understand the impact of poisoned data on machine learning models, specifically focusing on the Boston Housing dataset and a simple neural network architecture.\n",
    "\n",
    "Key takeaways from this lab include:\n",
    "\n",
    "1. **Baseline Performance**: Our initial model, trained on clean data, provided us with a baseline MSE of 14.27. This served as our reference point for subsequent experiments.\n",
    "\n",
    "2. **Impact of Poisoned Data**: Introducing poisoned data into our training set significantly degraded our model's performance, with the MSE rising to 23.49. This underscores the vulnerability of machine learning models to adversarial attacks and the importance of data integrity.\n",
    "\n",
    "3. **Detection and Mitigation**: Using the Isolation Forest algorithm, we were able to detect a majority of the poisoned data points with high precision and recall. After cleaning the dataset, retraining the model improved its performance, bringing the MSE closer to the baseline at 14.38.\n",
    "\n",
    "4. **Implications**: While the Isolation Forest was effective in detecting many poisoned data points, the slight increase in MSE after retraining suggests that some genuine data might have been misclassified as anomalies or other complexities in the training process affected the outcome. This highlights the challenges in perfectly restoring model performance post-attack and the importance of robust detection mechanisms.\n",
    "\n",
    "In conclusion, while machine learning models offer powerful capabilities, they are not immune to adversarial attacks. Ensuring data integrity, continuously monitoring model performance, and employing robust anomaly detection mechanisms are crucial steps in safeguarding our models against such threats.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
