{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "fc9f9a83-db3e-5b0a-a05f-11420f14235e",
        "openai_ephemeral_user_id": "54b6507c-3fa1-5e59-8349-a7d4331f354d",
        "openai_subdivision1_iso_code": "US-GA"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "identifier": "legacy",
      "language": "python",
      "language_version": "3.9",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "id": "a3c55c0e-1f00-4a4b-8a27-e7841b533ae1",
      "cell_type": "markdown",
      "source": [
        "# Federated Learning: Attack and Defense Demonstration\n",
        "\n",
        "In this notebook, we will show an attack on a federated learning environment as well as how to defend against it. Federated learning is a machine learning technique in which a model is trained on multiple devices. This approach is mainly used to ensure data privacy, and when it would be impossible to transfer data to a central server.\n",
        "\n",
        "However, it is possible to attack a federated learning environment, particularly through the use of malicious devices that introduce noisy data. In this demonstration, we will simulate this kind of attack and then introduce a defense mechanism to mitigate it."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "a3c55c0e-1f00-4a4b-8a27-e7841b533ae1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraries"
      ],
      "metadata": {
        "id": "FeObBKDAjXYX"
      },
      "id": "FeObBKDAjXYX"
    },
    {
      "id": "ca619241-e4cf-4d0f-a113-c587194903c7",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "ca619241-e4cf-4d0f-a113-c587194903c7"
      },
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define a simple linear regression model\n",
        "class LinearRegression:\n",
        "    def __init__(self, learning_rate=0.0001, weight=None, bias=None):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight = weight if weight is not None else np.random.randn(10)  # Initialize weight as a vector of size 10\n",
        "        self.bias = bias if bias is not None else np.random.randn()\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.dot(self.weight, x) + self.bias\n",
        "\n",
        "    def update(self, gradient_weight, gradient_bias):\n",
        "        self.weight -= self.learning_rate * gradient_weight\n",
        "        self.bias -= self.learning_rate * gradient_bias\n",
        "\n",
        "    def mse(self, data):\n",
        "        total_error = 0\n",
        "        for x, y in data:\n",
        "            y_pred = self.predict(x)\n",
        "            total_error += (y - y_pred) ** 2\n",
        "        return total_error / len(data)"
      ],
      "outputs": []
    },
    {
      "id": "69aa3f7a-b029-45e4-ba85-eebffe61cd72",
      "cell_type": "markdown",
      "source": [
        "## Linear Regression Model\n",
        "We defined a simple linear regression model with the following components:\n",
        "- **Initialization**: The model has a learning rate, weights, and bias. The weight vector is size 10.\n",
        "- **Predict**: This function will predict the output for an input `x` using $y = wx + b$, where `w` is the weight and `b` is the bias.\n",
        "- **Update**: This function will update the model's weights and bias based on the gradients. The learning rate is responsible for the step size for the update.\n",
        "- **Mean Squared Error (MSE)**: This function calculates the mean squared error for the given data. It's used to show the model's performance - lower values mean better performance.\n",
        "\n",
        "\n",
        "Define functions to simulate local training on devices and federated learning:"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "69aa3f7a-b029-45e4-ba85-eebffe61cd72"
      }
    },
    {
      "id": "f29f31b0-f450-45be-bfdf-3f46fb7e2367",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "f29f31b0-f450-45be-bfdf-3f46fb7e2367"
      },
      "execution_count": null,
      "source": [
        "# Simulate local training on devices\n",
        "def local_training(model, data, epochs=100):\n",
        "    for _ in range(epochs):\n",
        "        gradient_weight = 0\n",
        "        gradient_bias = 0\n",
        "        for x, y in data:\n",
        "            y_pred = model.predict(x)\n",
        "            gradient_weight += -2 * x * (y - y_pred)\n",
        "            gradient_bias += -2 * (y - y_pred)\n",
        "        model.update(gradient_weight, gradient_bias)\n",
        "\n",
        "# Federated learning simulation\n",
        "def federated_learning(global_model, local_data_list, epochs=100):\n",
        "    num_devices = len(local_data_list)\n",
        "    for _ in range(epochs):\n",
        "        global_gradient_weight = 0\n",
        "        global_gradient_bias = 0\n",
        "        for local_data in local_data_list:\n",
        "            local_model = LinearRegression(weight=global_model.weight, bias=global_model.bias)\n",
        "            local_training(local_model, local_data)\n",
        "            global_gradient_weight += (local_model.weight - global_model.weight) / num_devices  # Average difference\n",
        "            global_gradient_bias += (local_model.bias - global_model.bias) / num_devices  # Average difference\n",
        "        global_model.update(global_gradient_weight, global_gradient_bias)\n",
        "\n",
        "# Generate random data for devices\n",
        "def generate_data(num_points, x_range, y_range):\n",
        "    x = np.random.uniform(x_range[0], x_range[1], num_points)\n",
        "    y = np.random.uniform(y_range[0], y_range[1], num_points)\n",
        "    return list(zip(x, y))"
      ],
      "outputs": []
    },
    {
      "id": "184edcb8-6e34-41f6-8976-5e82c148544e",
      "cell_type": "markdown",
      "source": [
        "## Local Training and Federated Learning\n",
        "### Local Training\n",
        "The `local_training` function simulates the training of a model on a local device. Steps:\n",
        "\n",
        "1. For each epoch, initialize the gradients for weight and bias to 0.\n",
        "2. For each data point, predict the output and calculate the gradients for weight and bias based on the error.\n",
        "3. After processing all of the data points, it will update the model's weight and bias using the total gradients.\n",
        "\n",
        "### Federated Learning\n",
        "The `federated_learning` function simulates the federated learning process. Steps:\n",
        "\n",
        "1. For each epoch, initialize the global gradients for weight and bias to zero.\n",
        "2. For each local dataset (representing a device's data):\n",
        "   - Create a local model with the same weight and bias as the global model.\n",
        "   - Local model is trained using the `local_training` function.\n",
        "   - The difference between the local model's weight and bias and the global model's weight and bias is found and added to the global gradients. This difference is then averaged over the number of devices.\n",
        "3. After processing all local datasets, the global model's weight and bias are updated using the accumulated global gradients.\n",
        "\n",
        "### Data Generation\n",
        "The `generate_data` function simulates the generation of random data for the devices. It creates random `x` and `y` values within the specified ranges and returns them.\n",
        "\n",
        "Next, we'll load a real dataset, preprocess it, and simulate the federated learning process:"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "184edcb8-6e34-41f6-8976-5e82c148544e"
      }
    },
    {
      "id": "c065b213-8f09-499f-bee7-9a4e1bf86271",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "c065b213-8f09-499f-bee7-9a4e1bf86271"
      },
      "execution_count": null,
      "source": [
        "# Load the diabetes dataset\n",
        "diabetes = datasets.load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into chunks to simulate data on different devices\n",
        "num_devices = 4\n",
        "device_data = []\n",
        "for _ in range(num_devices):\n",
        "    X_train, X, y_train, y = train_test_split(X, y, test_size=0.75)\n",
        "    device_data.append(list(zip(X_train, y_train)))\n",
        "\n",
        "# Initialize a global model\n",
        "global_model = LinearRegression()\n",
        "\n",
        "# Evaluate model's metrics before federated learning\n",
        "all_data = [item for sublist in device_data for item in sublist]\n",
        "initial_mse = global_model.mse(all_data)\n",
        "print(f'Initial MSE: {initial_mse}')\n",
        "\n",
        "# Perform federated learning with the devices\n",
        "federated_learning(global_model, device_data)\n",
        "\n",
        "# Evaluate model's metrics after federated learning\n",
        "post_devices_mse = global_model.mse(all_data)\n",
        "print(f'MSE after training: {post_devices_mse}')"
      ],
      "outputs": []
    },
    {
      "id": "6849b54c-e26b-4606-a429-1c4ffd4170c4",
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing and Initial Federated Learning\n",
        "In the above code, we perform the following steps:\n",
        "\n",
        "1. **Load the Diabetes Dataset**: The diabetes dataset from the `sklearn.datasets` module. This dataset has 10 baseline variables such as age, sex, BMI, average blood pressure, and six blood serum measurements for 442 diabetes patients. The target variable is a measurement of the disease's progress one year after the baseline.\n",
        "\n",
        "2. **Data Standardization**: Standardize the dataset using the `StandardScaler` from `sklearn.preprocessing`.\n",
        "\n",
        "3. **Simulate Data on Different Devices**: Split the dataset into chunks to simulate the scenario in which the data is distributed across different devices.In other words, each device gets a subset of the data. This is a common practice in federated learning environemnents where each device (mobile phone, for example) has local data that isn't shared directly due to privacy concerns.\n",
        "\n",
        "4. **Initialize a Global Model**: Create an instance of our `LinearRegression` class, which acts as our global model.\n",
        "\n",
        "5. **Evaluate Model Before Training**: Before beginning federated learning process, we test the model's performance on all data using the Mean Squared Error (MSE). This gives us a baseline.\n",
        "\n",
        "6. **Federated Learning**: Perform federated learning using the `federated_learning` function. After training, we test the model's performance again to see the difference (improvement or degredation).\n",
        "\n",
        "The outputs show the model's MSE before and after federated learning.\n",
        "\n",
        "Next, we introduce an attack on the model by adding a noisy device and measure its impact on the model."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "6849b54c-e26b-4606-a429-1c4ffd4170c4"
      }
    },
    {
      "id": "e6d25e0a-f490-4ddc-87d6-4166733ddaf9",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "e6d25e0a-f490-4ddc-87d6-4166733ddaf9"
      },
      "execution_count": null,
      "source": [
        "# Introduce a fifth device with noisy data\n",
        "num_devices += 1\n",
        "X_noisy, _, y_noisy, _ = train_test_split(X, y, test_size=0.75)\n",
        "X_noisy += np.random.normal(0, 5, X_noisy.shape)  # Adding large noise to features\n",
        "y_noisy += np.random.normal(0, 100, y_noisy.shape)  # Adding large noise to targets\n",
        "device_data.append(list(zip(X_noisy, y_noisy)))\n",
        "\n",
        "# Re-evaluate the model with the new data\n",
        "federated_learning(global_model, device_data)\n",
        "\n",
        "# Evaluate model's metrics after federated learning with 5 devices\n",
        "all_data += list(zip(X_noisy, y_noisy))\n",
        "post_5_devices_mse = global_model.mse(all_data)\n",
        "print(f'MSE after poisoning: {post_5_devices_mse}')"
      ],
      "outputs": []
    },
    {
      "id": "23831bc9-079f-4cbf-b739-36fc777320e7",
      "cell_type": "markdown",
      "source": [
        "## Adversarial Attack: Introducing Noisy Data\n",
        "Above, we simulated an adversarial attack by introducing a 5th device with noisy data. Steps:\n",
        "\n",
        "1. **Introduce a Fifth Device** We add a new device to the simulation. This device will send noisy data to the model.\n",
        "\n",
        "2. **Generate Noisy Data**: For this device, we add a lot of noise to the features (`X_noisy`) and targets (`y_noisy`). This is meant to simulate a scenario in which someone attempts to sabotage the learning environment.\n",
        "\n",
        "3. **Re-evaluate the Model with Noisy Data**: We then redo the federated learning, including the noisy data.\n",
        "\n",
        "4. **Evaluate Model's Metrics After Poisoning**: After training, we measure the model's performance once again. This will show the effect that the noisy 5th device had on model performance\n",
        "\n",
        "Cearly based on the output, the model's performance suffered from the iintroduction of the noisy device."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "23831bc9-079f-4cbf-b739-36fc777320e7"
      }
    },
    {
      "id": "42ce2ff6-9667-4ec0-9960-a157142fbc47",
      "cell_type": "markdown",
      "source": [
        "## Averaged Federated Learning with Trust Scores\n",
        "Now we're goin gto implement a defense to this attack. The idea of the defense is to assign each model a trust score. This score will be used to judge the device's trustworthiness. This score will be updated based on how large of an update to the model the device has.\n",
        "\n",
        "Here's a step-by-step breakdown of the process:"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "42ce2ff6-9667-4ec0-9960-a157142fbc47"
      }
    },
    {
      "id": "6006bb7a-e3da-4967-a85f-24ab145db5c5",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "6006bb7a-e3da-4967-a85f-24ab145db5c5"
      },
      "execution_count": null,
      "source": [
        "# Redo but with averaged federated learning\n",
        "# Split the dataset into chunks to simulate data on different devices\n",
        "num_devices = 4\n",
        "device_data = []\n",
        "for _ in range(num_devices):\n",
        "    X_train, X, y_train, y = train_test_split(X, y, test_size=0.75)\n",
        "    device_data.append(list(zip(X_train, y_train)))\n",
        "\n",
        "# Initialize a global model\n",
        "global_model = LinearRegression()\n",
        "\n",
        "# Evaluate model's metrics before federated learning\n",
        "all_data = [item for sublist in device_data for item in sublist]\n",
        "initial_mse = global_model.mse(all_data)\n",
        "print(f'Initial MSE: {initial_mse}')"
      ],
      "outputs": []
    },
    {
      "id": "77dd02fe-ee85-43c2-aaaf-ce8eece64093",
      "cell_type": "markdown",
      "source": [
        "1. **Data Splitting for Devices:**\n",
        "   - Same as before.\n",
        "\n",
        "2. **Global Model Initialization:**\n",
        "   - Same as before.\n",
        "\n",
        "3. **Evaluation Before Training:**\n",
        "   - Same as before."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "77dd02fe-ee85-43c2-aaaf-ce8eece64093"
      }
    },
    {
      "id": "9eb7547b-5396-4ada-b23c-67b199f90730",
      "cell_type": "markdown",
      "source": [
        "### Federated Learning with Model Averaging\n",
        "\n",
        "This is where we introduce the model averaging and trust scores:"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "9eb7547b-5396-4ada-b23c-67b199f90730"
      }
    },
    {
      "id": "bcf6b5b4-d48a-432b-a62c-71f0ec894d5d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "bcf6b5b4-d48a-432b-a62c-71f0ec894d5d"
      },
      "execution_count": null,
      "source": [
        "# Federated learning simulation with model averaging\n",
        "def averaged_federated_learning(global_model, local_data_list, epochs=100):\n",
        "    for _ in range(epochs):\n",
        "        global_gradient_weight = 0\n",
        "        global_gradient_bias = 0\n",
        "        total_trust = sum(trust_scores)\n",
        "\n",
        "        for device_id, local_data in enumerate(local_data_list):\n",
        "            # Skip devices with trust score below threshold\n",
        "            if trust_scores[device_id] < trust_threshold:\n",
        "                continue\n",
        "\n",
        "            local_model = LinearRegression(weight=global_model.weight, bias=global_model.bias)\n",
        "            local_training(local_model, local_data)\n",
        "\n",
        "            # Weighted averaging using trust scores\n",
        "            weight_diff = (local_model.weight - global_model.weight)\n",
        "            bias_diff = (local_model.bias - global_model.bias)\n",
        "\n",
        "            global_gradient_weight += (weight_diff * trust_scores[device_id]) / total_trust\n",
        "            global_gradient_bias += (bias_diff * trust_scores[device_id]) / total_trust\n",
        "\n",
        "            # Adjust trust scores based on the magnitude of the update\n",
        "            if np.linalg.norm(weight_diff) > 0.05:  # Example threshold\n",
        "                trust_scores[device_id] *= 0.8  # Decrease trust score\n",
        "            else:\n",
        "                trust_scores[device_id] *= 1.05  # Increase trust score, but ensure it doesn't grow indefinitely\n",
        "\n",
        "        global_model.update(global_gradient_weight, global_gradient_bias)"
      ],
      "outputs": []
    },
    {
      "id": "09282cfa-65d5-4b0d-9b70-a68062a44cdd",
      "cell_type": "markdown",
      "source": [
        "4. **Model Averaging Based on Trust Scores:**\n",
        "   - This time, instead of simply averaging the updates sent to the model, we add weight to each update based on the trust score of the device sending it.\n",
        "   - All devices initially have a trust score of 1.0 - the default is to trust.\n",
        "   - When the model aggregates the updates, it will do so using a weighted average.\n",
        "   - If the magnitude of the model update from a device is greater than a certain number, (in this case, 0.05), the trust score of that device will be lowered. This is because a large update may be linked to the fact that that device is far off from the others, indicating the presence of noisy data. On the other hand, if the update is small, the trust score is slightly increased.\n",
        "   - Devices with low trust scores (0.5) are deemed untrustworthy and are ignored in future updates.\n",
        "\n",
        "5. **Training with Averaged Federated Learning:**\n",
        "   - Retrain the global model and evaluate."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "09282cfa-65d5-4b0d-9b70-a68062a44cdd"
      }
    },
    {
      "id": "0673b708-711f-4288-be31-9682d1e17b9d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "0673b708-711f-4288-be31-9682d1e17b9d"
      },
      "execution_count": null,
      "source": [
        "# Initialize trust scores for all devices\n",
        "trust_scores = [1.0 for _ in range(num_devices)]\n",
        "trust_threshold = 0.5  # Threshold below which a device is considered untrustworthy\n",
        "\n",
        "# Perform federated learning with the devices\n",
        "averaged_federated_learning(global_model, device_data)\n",
        "\n",
        "# Evaluate model's metrics after federated learning\n",
        "post_devices_mse = global_model.mse(all_data)\n",
        "print(f'MSE after averaging: {post_devices_mse}')"
      ],
      "outputs": []
    },
    {
      "id": "17eba2fd-5490-4744-bde7-871c5afe0219",
      "cell_type": "markdown",
      "source": [
        "6. **Introducing the Noisy Device**\n",
        "\n"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "17eba2fd-5490-4744-bde7-871c5afe0219"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce a fifth device with noisy data\n",
        "X_noisy, _, y_noisy, _ = train_test_split(X, y, test_size=0.75)\n",
        "X_noisy += np.random.normal(0, 5, X_noisy.shape)  # Adding large noise to features\n",
        "y_noisy += np.random.normal(0, 100, y_noisy.shape)  # Adding large noise to targets\n",
        "device_data.append(list(zip(X_noisy, y_noisy)))\n",
        "# Add a trust score for the new device\n",
        "trust_scores.append(1.0)"
      ],
      "metadata": {
        "id": "Ln2c_14snuBc"
      },
      "id": "Ln2c_14snuBc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Training with the Noisy Device:**"
      ],
      "metadata": {
        "id": "QjsPr3o1nupj"
      },
      "id": "QjsPr3o1nupj"
    },
    {
      "id": "be160d48-82df-47f1-91cd-39f54eb99030",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "be160d48-82df-47f1-91cd-39f54eb99030"
      },
      "execution_count": null,
      "source": [
        "\n",
        "\n",
        "# Re-evaluate the model with the poisoned data but with averaged federated learning\n",
        "averaged_federated_learning(global_model, device_data)\n",
        "\n",
        "# Evaluate model's metrics after federated learning with 5 devices\n",
        "all_data += list(zip(X_noisy, y_noisy))\n",
        "post_5_devices_mse = global_model.mse(all_data)\n",
        "print(f'MSE after poisoning but with averaged federated learning: {post_5_devices_mse}')"
      ],
      "outputs": []
    },
    {
      "id": "356168df-d85b-4c94-bd1d-e1757acb9f5e",
      "cell_type": "markdown",
      "source": [
        "8. **Analysis and Conclusion:**\n",
        "   - The results show the ability of the federated learning environment to protect from malicious devices.\n",
        "   - The initial MSE prior to the introduction of the noisy 5th device and after introducing federated learning were `29482.5` and `26469.9` respectively. this shows the model benefitted from the introduction of federated learning.\n",
        "   - After introducing the noisy 5th device and re-training, the MSE was `29356.4`. This value is higher than the initial MSE. This means that the model's performance was worsened by this malicious device.\n",
        "   - The introduction of trust scores, though, was able to mitigate this negative impact of the noise.\n",
        "   - When re-instantiated, the initial model had an MSE of `28427.9`. After the first round of averaged federated learning the model's MSE was `26383.7`. These are very similar results to the first time through.\n",
        "   - However, when the model was poisoned but trained using the trust score system, the MSE was `23259.9`. This is the ***lowest MSE of all***.\n",
        "   - This suggests that this approach was not only able to defend against the attack,, but actually improved the model's overall performance."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "356168df-d85b-4c94-bd1d-e1757acb9f5e"
      }
    }
  ]
}